"""
å¢å¼ºç‰ˆå®ç›˜é€‰è‚¡æ¨è v3.0
é›†æˆå¤šä¿¡å·éªŒè¯ã€å¸‚åœºè‡ªé€‚åº”ã€è´¢åŠ¡ç­›é€‰
Date: 2025-12-24

ä½¿ç”¨æ–¹æ³•:
    python run_live_strategy_enhanced.py
    python run_live_strategy_enhanced.py --date 20241220
    python run_live_strategy_enhanced.py --lookback 90 --debug
"""

import sys
import argparse
import logging
from pathlib import Path
from datetime import datetime, timedelta
import pandas as pd
import joblib

# å¯¼å…¥åŸæœ‰æ¨¡å—
from config import Config
from data_manager import DataManager
from factor_engine import FactorEngine
from ml_model import MLModel, WalkForwardTrainer

# ã€å…³é”®ã€‘å¯¼å…¥å¢å¼ºç­–ç•¥
from enhanced_live_strategy import EnhancedLiveStrategy

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('enhanced_live.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


def get_latest_available_trading_date(cache_dir, lookback=5):
    """
    æ™ºèƒ½è·å–æœ€æ–°å¯ç”¨äº¤æ˜“æ—¥æœŸ
    """
    from datetime import datetime, timedelta

    today = datetime.now()

    # å¦‚æœæ˜¯å‘¨æœ«ï¼Œå¾€å‰æ¨
    if today.weekday() >= 5:  # å‘¨å…­(5)æˆ–å‘¨æ—¥(6)
        days_back = today.weekday() - 4  # æ¨åˆ°å‘¨äº”
        today = today - timedelta(days=days_back)

    # å¦‚æœæ˜¯äº¤æ˜“æ—¥çš„æ—©ç›˜å‰ï¼Œä½¿ç”¨å‰ä¸€å¤©
    if today.hour < 15:
        today = today - timedelta(days=1)

    latest_date = today.strftime('%Y%m%d')
    return latest_date, True, "å®æ—¶"


def main():
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆå®ç›˜é€‰è‚¡æ¨èç³»ç»Ÿ v3.0')
    parser.add_argument('--date', type=str, help='æŒ‡å®šæ—¥æœŸ (æ ¼å¼: YYYYMMDD)')
    parser.add_argument('--lookback', type=int, default=150, help='å›çœ‹å¤©æ•° (å»ºè®®150å¤©ä»¥ä¸Š)')
    parser.add_argument('--debug', action='store_true', help='è°ƒè¯•æ¨¡å¼')
    args = parser.parse_args()

    print("\n" + "=" * 80)
    print("ğŸš€ å¢å¼ºç‰ˆå®ç›˜é€‰è‚¡æ¨èç³»ç»Ÿ v3.0")
    print("=" * 80)
    print("æ ¸å¿ƒåŠŸèƒ½:")
    print("  âœ… å¤šä¿¡å·äº¤å‰éªŒè¯ (4å±‚é‡‘å­—å¡”)")
    print("  âœ… æœ€ä½³ä¹°å…¥æ—¶ç‚¹è¯†åˆ«")
    print("  âœ… è´¢åŠ¡è´¨é‡æ·±åº¦ç­›æŸ¥")
    print("  âœ… å¸‚åœºç¯å¢ƒè‡ªé€‚åº”")
    print("  âœ… åŠ¨æ€ä»“ä½åˆ†é…")
    print("  âœ… è¡Œä¸šè½®åŠ¨æ•æ‰")
    print("=" * 80 + "\n")

    # ===== 1. åˆå§‹åŒ–ç³»ç»Ÿ =====
    logger.info("åˆå§‹åŒ–ç³»ç»Ÿ...")

    config = Config()
    config.data.use_cache = True

    dm = DataManager(config)
    fe = FactorEngine(config)
    trainer = WalkForwardTrainer(config)
    enhanced_strategy = EnhancedLiveStrategy(config)  # ã€å…³é”®ã€‘å¢å¼ºç­–ç•¥

    # ===== 2. ç¡®å®šæ—¥æœŸ =====
    if args.date:
        latest_date = args.date
        is_real_time = False
        data_source = "ç”¨æˆ·æŒ‡å®š"
        logger.info(f"âœ“ ä½¿ç”¨æŒ‡å®šæ—¥æœŸ: {latest_date}")
    else:
        latest_date, is_real_time, data_source = get_latest_available_trading_date(
            config.data.cache_dir
        )
        logger.info(f"âœ“ è‡ªåŠ¨é€‰æ‹©æ—¥æœŸ: {latest_date} (æ¥æº: {data_source})")

    # ===== 3. è·å–æ•°æ® =====
    # ã€ä¼˜å…ˆä½¿ç”¨ç¼“å­˜ã€‘
    cache_dir = Path(config.data.cache_dir)
    cache_files = list(cache_dir.glob('daily_all_*.pkl'))

    if cache_files:
        latest_cache = max(cache_files, key=lambda p: p.stat().st_mtime)
        logger.info(f"ä½¿ç”¨ç¼“å­˜: {latest_cache.name}")
        
        df_history = pd.read_pickle(latest_cache)
        
        # ä½¿ç”¨ç¼“å­˜ä¸­çš„æœ€æ–°æ—¥æœŸ
        latest_date = df_history['trade_date'].max()
        logger.info(f"âœ“ ç¼“å­˜æœ€æ–°æ—¥æœŸ: {latest_date}")
        
        # ç¡®ä¿start_dateæ˜¯å¯ç”¨çš„
        start_date = (
            datetime.strptime(latest_date, '%Y%m%d') - timedelta(days=args.lookback)
        ).strftime('%Y%m%d')
    else:
        # åŸæ¥çš„è·å–é€»è¾‘
        try:
            start_date = (
                datetime.strptime(latest_date, '%Y%m%d') - timedelta(days=args.lookback)
            ).strftime('%Y%m%d')
        except ValueError:
            logger.error(f"âŒ æ—¥æœŸæ ¼å¼é”™è¯¯: {latest_date}")
            return

        logger.info(f"\n{'=' * 60}")
        logger.info(f"ğŸ“Š æ•°æ®å‡†å¤‡: {start_date} ~ {latest_date}")
        logger.info(f"{'=' * 60}\n")

        try:
            df_history = dm.get_daily_data(start_date=start_date, end_date=latest_date)
            logger.info(f"âœ“ è·å–æ•°æ®: {len(df_history)} æ¡")
        except Exception as e:
            logger.error(f"âŒ æ•°æ®è·å–å¤±è´¥: {e}")
            return

    # æ·»åŠ è¡Œä¸šä¿¡æ¯
    try:
        industry_df = dm.get_industry_data()
        df_history = df_history.merge(
            industry_df[['ts_code', 'industry']],
            on='ts_code',
            how='left'
        )
        logger.info(f"âœ“ æ·»åŠ è¡Œä¸šä¿¡æ¯")
    except Exception as e:
        logger.warning(f"âš ï¸  æ— æ³•è·å–è¡Œä¸šä¿¡æ¯: {e}")

    if latest_date not in df_history['trade_date'].values:
        logger.error(f"âŒ æ•°æ®ä¸­ä¸åŒ…å« {latest_date}")
        available_dates = sorted(df_history['trade_date'].unique())[-5:]
        logger.info(f"å¯ç”¨æ—¥æœŸ: {available_dates}")
        logger.info(f"å°è¯•: python {sys.argv[0]} --date {available_dates[-1]}")
        return

    # ===== 4. è®¡ç®—å› å­ =====
    logger.info(f"\n{'=' * 60}")
    logger.info("âš™ï¸  è®¡ç®—å› å­ç‰¹å¾...")
    logger.info(f"{'=' * 60}\n")

    try:
        df_with_factors = fe.calculate_all_factors(df_history)
        logger.info(f"âœ“ å› å­è®¡ç®—å®Œæˆ")
    except Exception as e:
        logger.error(f"âŒ å› å­è®¡ç®—å¤±è´¥: {e}")
        if args.debug:
            import traceback
            traceback.print_exc()
        return

    df_latest = df_with_factors[df_with_factors['trade_date'] == latest_date].copy()

    if df_latest.empty:
        logger.error(f"âŒ å› å­è®¡ç®—å {latest_date} æ•°æ®ä¸ºç©º")
        return

    logger.info(f"âœ“ å½“æ—¥å€™é€‰è‚¡ç¥¨: {len(df_latest)} åª")

    # ===== 5. åŠ è½½æ¨¡å‹ =====
    logger.info(f"\n{'=' * 60}")
    logger.info("ğŸ¤– åŠ è½½æœºå™¨å­¦ä¹ æ¨¡å‹...")
    logger.info(f"{'=' * 60}\n")

    model_path = Path(config.data.cache_dir) / 'latest_model.pkl'
    if not model_path.exists():
        model_path = Path('latest_model.pkl')

    if not model_path.exists():
        logger.error("âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
        logger.error("è¯·å…ˆè¿è¡Œ: python main.py --mode train")
        return

    try:
        model = MLModel(config)
        model.model = joblib.load(str(model_path))
        logger.info(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")

        # ã€å…³é”®ä¿®å¤ã€‘è·å–æ¨¡å‹è®­ç»ƒæ—¶ä½¿ç”¨çš„ç‰¹å¾åˆ—è¡¨
        if hasattr(model.model, 'feature_names_in_'):
            expected_features = list(model.model.feature_names_in_)
        elif hasattr(model.model, 'feature_name_'):
            expected_features = model.model.feature_name_
        else:
            logger.error("âŒ æ— æ³•è·å–æ¨¡å‹ç‰¹å¾åˆ—è¡¨")
            return
        
        logger.info(f"   æ¨¡å‹éœ€è¦ {len(expected_features)} ä¸ªç‰¹å¾")
        
        # æ£€æŸ¥å½“å‰æ•°æ®æœ‰å“ªäº›ç‰¹å¾
        current_features = set(df_latest.columns)
        missing_features = set(expected_features) - current_features
        
        if missing_features:
            logger.warning(f"   ç¼ºå¤± {len(missing_features)} ä¸ªç‰¹å¾ï¼Œå°†è‡ªåŠ¨ç”Ÿæˆ")
            
            # ä¸ºç¼ºå¤±çš„æ»åç‰¹å¾å¡«å……
            for feat in missing_features:
                if feat.endswith('_lag1'):
                    # æ»åç‰¹å¾ï¼šå°è¯•ä»å‰ä¸€å¤©è·å–
                    base_feat = feat[:-5]  # å»æ‰ '_lag1'
                    if base_feat in df_with_factors.columns:
                        # ä»å†å²æ•°æ®è·å–å‰ä¸€å¤©çš„å€¼
                        df_latest[feat] = df_with_factors.groupby('ts_code')[base_feat].shift(1)
                    else:
                        df_latest[feat] = 0
                else:
                    # å…¶ä»–ç¼ºå¤±ç‰¹å¾å¡«0
                    df_latest[feat] = 0
        
        # ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½å­˜åœ¨ä¸”æŒ‰æ­£ç¡®é¡ºåº
        X = pd.DataFrame(df_latest[expected_features].copy())
        
        # å¡«å……NaNï¼ˆæ»åç‰¹å¾çš„ç¬¬ä¸€å¤©ä¼šæ˜¯NaNï¼‰
        X = X.fillna(0)
        
        # é¢„æµ‹
        preds = model.predict(X)
        
        # ç¡®ä¿predsæ˜¯numpyæ•°ç»„ä»¥æ”¯æŒåç»­æ“ä½œ
        import numpy as np
        if not isinstance(preds, np.ndarray):
            preds = np.array(preds)
        
        df_latest['ml_score'] = preds

        logger.info(f"âœ“ è¯„åˆ†å®Œæˆ")
        logger.info(f"   è¯„åˆ†èŒƒå›´: {preds.min():.3f} ~ {preds.max():.3f}")
        logger.info(f"   å¹³å‡è¯„åˆ†: {preds.mean():.3f}")
        logger.info(f"   é«˜åˆ†è‚¡ç¥¨(>0.6): {(preds > 0.6).sum()} åª")

    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
        if args.debug:
            import traceback
            traceback.print_exc()
        return

    # ===== 6. è·å–æŒ‡æ•°æ•°æ® =====
    logger.info(f"\n{'=' * 60}")
    logger.info("ğŸ“ˆ è·å–æŒ‡æ•°æ•°æ®...")
    logger.info(f"{'=' * 60}\n")

    try:
        index_data = dm.get_index_data(
            config.backtest.benchmark,
            start_date
        )
        logger.info(f"âœ“ æŒ‡æ•°æ•°æ®è·å–æˆåŠŸ")
    except Exception as e:
        logger.warning(f"âš ï¸  æŒ‡æ•°æ•°æ®è·å–å¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤ç­–ç•¥: {e}")
        index_data = None

    # ===== 7. å¢å¼ºé€‰è‚¡ï¼ˆæ ¸å¿ƒï¼‰ =====
    logger.info(f"\n{'=' * 60}")
    logger.info("ğŸ¯ æ‰§è¡Œå¢å¼ºç‰ˆé€‰è‚¡ç­–ç•¥...")
    logger.info(f"{'=' * 60}\n")

    try:
        selected = enhanced_strategy.select_stocks_enhanced(
            df_date=df_latest,
            df_history=df_with_factors,
            index_data=index_data
        )
    except Exception as e:
        logger.error(f"âŒ é€‰è‚¡å¤±è´¥: {e}")
        if args.debug:
            import traceback
            traceback.print_exc()
        return

    # ===== 8. è¾“å‡ºç»“æœ =====
    print("\n" + "=" * 100)
    print(f"ğŸ“‹ {latest_date} å¢å¼ºç‰ˆå®ç›˜é€‰è‚¡æ¨è")
    print(f"   æ•°æ®æ¥æº: {data_source}")
    print(f"   ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 100 + "\n")

    if selected.empty:
        print("âš ï¸  ä»Šæ—¥æ— æ¨èè‚¡ç¥¨")
        print("\nå¯èƒ½åŸå› :")
        print("  - å¸‚åœºæ•´ä½“è¯„åˆ†è¾ƒä½")
        print("  - æœªæ‰¾åˆ°ç¬¦åˆå¤šé‡éªŒè¯çš„æ ‡çš„")
        print("  - å¸‚åœºç¯å¢ƒä¸ä½³ï¼Œç³»ç»Ÿè‡ªåŠ¨é™ä½æ¨èæ•°é‡")
        print("  - é£é™©æ§åˆ¶è§¦å‘é™åˆ¶\n")
    else:
        # æ ¼å¼åŒ–æ˜¾ç¤º
        print(f"{'ä»£ç ':10s} {'åç§°':10s} {'ä»·æ ¼':>8s} {'æ¶¨è·Œ':>8s} "
              f"{'ç»¼åˆè¯„åˆ†':>8s} {'çº§åˆ«':>4s} {'ä»“ä½':>6s} {'å¸‚åœºçŠ¶æ€':>10s} {'æ¨èç†ç”±':40s}")
        print("-" * 100)

        for _, row in selected.iterrows():
            print(f"{row['ts_code']:10s} "
                  f"{row.get('name', 'N/A'):10s} "
                  f"{row['close']:8.2f} "
                  f"{row.get('pct_chg', 0):+7.2f}% "
                  f"{row.get('composite_score', row.get('ml_score', 0)):8.3f} "
                  f"{row.get('position_tier', 'B'):>4s} "
                  f"{row.get('weight', 0) * 100:5.2f}% "
                  f"{row.get('market_state', 'unknown'):>10s} "
                  f"{row.get('recommendation', 'è¯„åˆ†è‰¯å¥½')[:40]:40s}")

        print("=" * 100)

        # ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   æ¨èæ•°é‡: {len(selected)} åª")
        print(f"   å¸‚åœºçŠ¶æ€: {selected.iloc[0].get('market_state', 'unknown')}")

        if 'composite_score' in selected.columns:
            print(f"   å¹³å‡è¯„åˆ†: {selected['composite_score'].mean():.3f}")

        if 'position_tier' in selected.columns:
            for tier in ['S', 'A', 'B', 'C']:
                count = (selected['position_tier'] == tier).sum()
                if count > 0:
                    print(f"   {tier}çº§è‚¡ç¥¨: {count} åª")

        # è¡Œä¸šåˆ†å¸ƒ
        if 'industry' in selected.columns:
            industry_counts = selected['industry'].value_counts()
            print(f"\nğŸ“ˆ è¡Œä¸šåˆ†å¸ƒ:")
            for ind, cnt in industry_counts.items():
                print(f"   {ind}: {cnt} åª")

        print()

    # ===== 9. ä¿å­˜ç»“æœ =====
    output_dir = Path('output/enhanced_live_recommendations')
    output_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime('%H%M%S')

    if not selected.empty:
        # CSV
        csv_path = output_dir / f'recommendations_{latest_date}_{timestamp}.csv'
        selected.to_csv(csv_path, index=False, encoding='utf-8-sig')
        logger.info(f"\nâœ“ CSVç»“æœå·²ä¿å­˜: {csv_path}")

        # Excel
        try:
            excel_path = output_dir / f'recommendations_{latest_date}_{timestamp}.xlsx'
            with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
                selected.to_excel(writer, sheet_name='æ¨èè‚¡ç¥¨', index=False)
            logger.info(f"âœ“ ExcelæŠ¥å‘Šå·²ä¿å­˜: {excel_path}")
        except Exception as e:
            logger.warning(f"âš ï¸  Excelä¿å­˜å¤±è´¥: {e}")

    print(f"\n{'=' * 100}")
    print("âœ… ç¨‹åºæ‰§è¡Œå®Œæˆ")
    print(f"{'=' * 100}\n")


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        logger.error(f"ç¨‹åºå¼‚å¸¸: {e}", exc_info=True)
        raise