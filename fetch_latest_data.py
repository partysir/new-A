"""
ä¼˜åŒ–æ•°æ®è·å–è„šæœ¬
è§£å†³Tushareé™æµé—®é¢˜

ç­–ç•¥:
1. åªè·å–æœ€æ–°1å¤©æ•°æ®ï¼ˆå¢é‡æ›´æ–°ï¼‰
2. æ™ºèƒ½é‡è¯•å’Œé”™è¯¯æ¢å¤
3. ä¼˜å…ˆä½¿ç”¨ç¼“å­˜

ä½¿ç”¨æ–¹æ³•:
    python fetch_latest_data.py
"""

import logging
from pathlib import Path
from datetime import datetime, timedelta
import pandas as pd
import time

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(message)s'
)
logger = logging.getLogger(__name__)


def get_latest_cache():
    """è·å–æœ€æ–°ç¼“å­˜æ–‡ä»¶"""
    cache_dir = Path('data_cache')

    if not cache_dir.exists():
        return None, None

    cache_files = list(cache_dir.glob('daily_all_*.pkl'))

    if not cache_files:
        return None, None

    latest_cache = max(cache_files, key=lambda p: p.stat().st_mtime)

    # è§£ææ—¥æœŸ
    import re
    match = re.search(r'_(\d{8})\.pkl', latest_cache.name)
    if match:
        cache_date = match.group(1)
        return latest_cache, cache_date

    return latest_cache, None


def fetch_single_day(ts_api, date):
    """è·å–å•æ—¥æ•°æ®ï¼ˆå¸¦é‡è¯•ï¼‰"""
    max_retries = 3
    retry_delay = 60  # ç§’

    for attempt in range(max_retries):
        try:
            df = ts_api.daily(trade_date=date)

            if df is not None and not df.empty:
                logger.info(f"âœ“ è·å– {date}: {len(df)} æ¡è®°å½•")
                return df
            else:
                logger.warning(f"âš ï¸  {date} æ— æ•°æ®")
                return pd.DataFrame()

        except Exception as e:
            if 'exceeded' in str(e).lower() or 'limit' in str(e).lower():
                if attempt < max_retries - 1:
                    logger.warning(f"é™æµï¼Œç­‰å¾… {retry_delay} ç§’... (é‡è¯• {attempt + 1}/{max_retries})")
                    time.sleep(retry_delay)
                else:
                    logger.error(f"âŒ {date} è·å–å¤±è´¥: è¶…è¿‡é‡è¯•æ¬¡æ•°")
                    return None
            else:
                logger.error(f"âŒ {date} è·å–å¤±è´¥: {e}")
                return None

    return None


def update_cache_incremental():
    """å¢é‡æ›´æ–°ç¼“å­˜"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Š å¢é‡æ›´æ–°æ•°æ®")
    logger.info("=" * 60)

    # 1. åŠ è½½æ—§ç¼“å­˜
    cache_path, cache_date = get_latest_cache()

    if not cache_path:
        logger.error("âŒ æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶")
        logger.info("è¯·å…ˆè¿è¡Œå®Œæ•´è·å–:")
        logger.info("  python run_live_strategy_enhanced.py --lookback 30")
        return False

    logger.info(f"1. æ‰¾åˆ°ç¼“å­˜: {cache_path.name}")
    logger.info(f"   ç¼“å­˜æ—¥æœŸ: {cache_date}")

    df_old = pd.read_pickle(cache_path)
    logger.info(f"   æ—§æ•°æ®: {len(df_old)} æ¡")

    # 2. ç¡®å®šéœ€è¦æ›´æ–°çš„æ—¥æœŸ
    cache_dt = datetime.strptime(cache_date, '%Y%m%d')
    today = datetime.now()

    days_to_fetch = []
    current_dt = cache_dt + timedelta(days=1)

    while current_dt <= today:
        # è·³è¿‡å‘¨æœ«
        if current_dt.weekday() < 5:
            days_to_fetch.append(current_dt.strftime('%Y%m%d'))
        current_dt += timedelta(days=1)

    if not days_to_fetch:
        logger.info("âœ“ ç¼“å­˜å·²æ˜¯æœ€æ–°")
        return True

    logger.info(f"2. éœ€è¦æ›´æ–° {len(days_to_fetch)} å¤©")

    # 3. åˆå§‹åŒ–Tushare
    try:
        import tushare as ts
        from config import Config

        config = Config()
        ts.set_token(config.data.tushare_token)
        pro = ts.pro_api()

        logger.info("âœ“ Tushareå·²åˆå§‹åŒ–")
    except Exception as e:
        logger.error(f"âŒ Tushareåˆå§‹åŒ–å¤±è´¥: {e}")
        return False

    # 4. é€æ—¥è·å–
    new_data = []

    for i, date in enumerate(days_to_fetch, 1):
        logger.info(f"[{i}/{len(days_to_fetch)}] è·å– {date}...")

        df_day = fetch_single_day(pro, date)

        if df_day is not None and not df_day.empty:
            new_data.append(df_day)

        # é¿å…é™æµ
        if i < len(days_to_fetch):
            time.sleep(0.5)

    # 5. åˆå¹¶æ•°æ®
    if new_data:
        df_new = pd.concat(new_data, ignore_index=True)
        logger.info(f"âœ“ æ–°æ•°æ®: {len(df_new)} æ¡")

        df_combined = pd.concat([df_old, df_new], ignore_index=True)
        df_combined = df_combined.drop_duplicates(
            subset=['ts_code', 'trade_date'],
            keep='last'
        )

        logger.info(f"âœ“ åˆå¹¶å: {len(df_combined)} æ¡")

        # 6. ä¿å­˜æ–°ç¼“å­˜
        latest_date = df_combined['trade_date'].max()
        new_cache_path = cache_path.parent / f'daily_all_{cache_date}_{latest_date}.pkl'

        df_combined.to_pickle(new_cache_path)
        logger.info(f"âœ“ å·²ä¿å­˜: {new_cache_path.name}")

        return True
    else:
        logger.warning("âš ï¸  æœªè·å–åˆ°æ–°æ•°æ®")
        return False


def quick_fetch_today():
    """å¿«é€Ÿè·å–ä»Šæ—¥æ•°æ®"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸš€ å¿«é€Ÿè·å–ä»Šæ—¥æ•°æ®")
    logger.info("=" * 60)

    # ç¡®å®šä»Šæ—¥æ—¥æœŸ
    today = datetime.now()

    # å¦‚æœæ˜¯å‘¨æœ«æˆ–æ—©ç›˜å‰ï¼Œä½¿ç”¨ä¸Šä¸€äº¤æ˜“æ—¥
    if today.weekday() >= 5:  # å‘¨æœ«
        days_back = today.weekday() - 4
        today = today - timedelta(days=days_back)
    elif today.hour < 15:  # æ”¶ç›˜å‰
        today = today - timedelta(days=1)

    today_str = today.strftime('%Y%m%d')

    logger.info(f"ç›®æ ‡æ—¥æœŸ: {today_str}")

    # åˆå§‹åŒ–Tushare
    try:
        import tushare as ts
        from config import Config

        config = Config()
        ts.set_token(config.data.tushare_token)
        pro = ts.pro_api()

        logger.info("âœ“ Tushareå·²åˆå§‹åŒ–")
    except Exception as e:
        logger.error(f"âŒ Tushareåˆå§‹åŒ–å¤±è´¥: {e}")
        return None

    # è·å–æ•°æ®
    df_today = fetch_single_day(pro, today_str)

    if df_today is not None and not df_today.empty:
        # ä¿å­˜
        output_path = Path('data_cache') / f'daily_{today_str}.pkl'
        output_path.parent.mkdir(exist_ok=True)

        df_today.to_pickle(output_path)
        logger.info(f"âœ“ å·²ä¿å­˜: {output_path}")

        return df_today
    else:
        logger.error("âŒ è·å–å¤±è´¥")
        return None


def main():
    print("\n" + "=" * 70)
    print(" " * 20 + "ğŸ“Š æ•°æ®è·å–å·¥å…·")
    print("=" * 70)

    print("""
é€‰æ‹©æ“ä½œ:

1. å¢é‡æ›´æ–°ï¼ˆæ¨èï¼‰
   - åªè·å–ç¼“å­˜åçš„æ–°æ•°æ®
   - é€Ÿåº¦å¿«ï¼Œä¸æ˜“é™æµ

2. å¿«é€Ÿè·å–ä»Šæ—¥
   - åªè·å–ä»Šå¤©1å¤©æ•°æ®
   - æœ€å¿«ï¼Œä½†ç¼“å­˜ä¸å®Œæ•´

3. æŸ¥çœ‹ç¼“å­˜çŠ¶æ€
   - æŸ¥çœ‹ç°æœ‰ç¼“å­˜ä¿¡æ¯

0. é€€å‡º
    """)

    choice = input("è¯·é€‰æ‹© (0-3): ").strip()

    if choice == '1':
        update_cache_incremental()

    elif choice == '2':
        df = quick_fetch_today()
        if df is not None:
            print(f"\nâœ“ è·å–æˆåŠŸ: {len(df)} æ¡è®°å½•")

    elif choice == '3':
        cache_path, cache_date = get_latest_cache()
        if cache_path:
            df = pd.read_pickle(cache_path)

            print("\n" + "=" * 60)
            print("ç¼“å­˜ä¿¡æ¯:")
            print("=" * 60)
            print(f"æ–‡ä»¶: {cache_path.name}")
            print(f"å¤§å°: {cache_path.stat().st_size / 1024 / 1024:.2f} MB")
            print(f"æ—¥æœŸ: {cache_date}")
            print(f"è®°å½•æ•°: {len(df)}")
            print(f"è‚¡ç¥¨æ•°: {df['ts_code'].nunique()}")
            print(f"æ—¥æœŸèŒƒå›´: {df['trade_date'].min()} ~ {df['trade_date'].max()}")

            days_old = (datetime.now() - datetime.strptime(cache_date, '%Y%m%d')).days
            print(f"è·ä»Š: {days_old} å¤©")

            if days_old == 0:
                print("âœ“ ç¼“å­˜æ˜¯æœ€æ–°çš„")
            elif days_old <= 3:
                print("âœ“ ç¼“å­˜è¾ƒæ–°")
            else:
                print("âš ï¸  ç¼“å­˜è¾ƒæ—§ï¼Œå»ºè®®æ›´æ–°")

            print("=" * 60)
        else:
            print("\nâŒ æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶")

    elif choice == '0':
        print("é€€å‡º")

    else:
        print("æ— æ•ˆé€‰æ‹©")


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nå·²å–æ¶ˆ")
    except Exception as e:
        logger.error(f"é”™è¯¯: {e}", exc_info=True)